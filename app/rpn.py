import os
import sys
import streamlit as st

from src.app_utils import plot_anchors


def rpn(session_state):

    st.title("Inline Region Proposal Network")

    #    TO DO
    # 1. explain how RPN works > anchors (sizes, aspects), sliding grids
    # 2. Add kpi's: image size, # anchor boxes from this pyramid level, stride size, anchor box details (sizes?)
    # 3. Explain classification and regression for each box.. then post processing.

    st.write(
        "Now that we've extracted features from the raw input image, the next step in the inference workflow is to generate a set of proposal \
        locations on the image that _may_ contain an object. Proposals are nothing more than candidate regions for the objects of interest that get \
        classified as object or not-object. But how can we capture the seemingly infinite object location, shape, and size possibilities that exist in an image?"
    )
    st.write(
        "Early approaches to object detection tackled this problem using region proposal algorithms like [selective search]() where image segmentation \
            was used to compute hierarchical groupings of similar regions based on pixel adjacency, color, texture, and size. While effective, this process \
        is computationally expensive and slow as it requires a dedicated offline algorithm to generate object proposals."
    )
    st.write(
        "To overcome these issues, the [Faster-RCNN](https://arxiv.org/pdf/1506.01497.pdf) design introduced the concept of a Region Proposal Network (RPN) \
            that takes advantage of shared feature maps extracted by the CNN backbone to propose object locations in a single, end-to-end network. Like many modern \
            detectors, RetinaNet adopts the concept of _anchor boxes_ as introduced by RPN."
    )

    with st.beta_expander("How do RPNs work?"):

        st.write("**Sliding-window Anchor Grid**")
        st.write(
            "RPN's generate proposals by overlaying a grid of anchor points onto a feature map output by the backbone network where each point corresponds to the center of one activation. \
            A set of anchor boxes is then slid over each point in the anchor grid to capture objects of variable size, height, and width. In particular, RetinaNet captures 9 anchor boxes per spatial \
            location by permuting the anchor box size and aspect ratios. This means that for the 6x6 feature map in the example below, the RPN will generate 6x6x9 = 324 object proposal boxes."
        )
        st.image("images/anchor_explain.png")
        st.write("**RPN for FPN**")
        st.write(
            "As learned in the previous step, RetinaNet uses an FPN with 5 pyramid levels to extract multi-scale features. To accommodate this, the RPN independently applys a separate anchor grid to the feature maps \
            generated by each level. The anchors have sizes ranging from 32$^2$ to 512$^2$ pixels across pyramid levels P$_3$ - P$_7$ respectively. This design allows anchor boxes from lower level (higher dimensional) feature maps \
            to capture smaller objects (by pixel size), while larger anchor boxes detect objects from higher level features."
        )
        st.write("**Classification and Box Regression Subnets**")
        st.write(
            "RetinaNet attaches two independent, fully convolutional networks to the outputs of each FPN level. Each subnet shares parameters across all levels. In the classification subnet, each anchor box is responsible \
            for detecting the existence of _at most_ one object from N classes in the spatial region the anchor box covers. Similarly, in the box regression subnet, each anchor box is responsible for detecting the size and \
            shape of _at most_ one object (if any exist) by regressing the relative offset of an object's bounding box from the anchor box."
        )
        pass

    with st.beta_expander("RPN Anchor Grid by Pyramid Level", expanded=True):

        st.write(
            "Given the explanation above, the following widget visualizes the anchor grid and anchor box sizes that are applied to feature maps at each FPN level. The top image overlays a set of 9 anchor boxes centered at _one_ anchor point for \
            each object in the image. The bottom image shows a sampled feature map from the selected FPN level to help visualize the activation granularity. By toggling the slider, we can infer which level of the FPN was likely used to detect \
            each object."
        )

        pyramid_level = st.select_slider(
            label="Select a Feature Pyramid Level:",
            options=[f"P{i+3}" for i in range(5)],
        )

        stats = session_state.data_artifacts["anchor_plots"][pyramid_level]["fig_stats"]

        col1, col2 = st.beta_columns(2)
        with col1:
            st.info(
                f'**Image Size:** ({" x ".join([str(stat) for stat in stats["image_size"]])}) px \n\n **Anchor Grid:** ({" x ".join([str(stat) for stat in stats["grid_size"]])}) cells \n\n **Total # Anchors:** {stats["grid_size"][0]*stats["grid_size"][1]*9}'
            )
        with col2:
            st.info(
                f'**Anchor Sizes:** {stats["anchor_sizes"]} px$^2$ \n\n **Anchor Stride:** ({" x ".join([str(stat) for stat in stats["stride"]])}) px'
            )

        st.image(session_state.fig_paths["rpn"][pyramid_level])

    return
