import os
import sys
import streamlit as st

from src.app_utils import plot_anchors


def anchor_boxs(session_state):

    st.title("Efficient Region Proposal Network")

    #    TO DO
    # 1. explain how RPN works > anchors (sizes, aspects), sliding grids
    # 2. Add kpi's: image size, # anchor boxes from this pyramid level, stride size, anchor box details (sizes?)
    # 3. Explain classification and regression for each box.. then post processing.

    st.write(
        "Now that we've extracted features from the raw input image, the next step in the inference workflow is to generate a set of proposal \
             locations in the image that _may_ contain an object. Those object proposals can then be classified as object or not-object. But how can \
            we capture the seemingly infinite object location, shape, and size possibilities?"
    )
    st.write(
        "Early approaches to object detection tackled this problem using region proposal algorithms like [selective search]() where image segmentation \
            was used to compute hierarchical groupings of similar regions based on pixel adjacency, color, texture, and size. While effective, this process \
        is computationally expensive and slow because it requires a dedicated offline algorithm to generate object proposals."
    )
    st.write(
        "To overcome these issues, the [Faster-RCNN](https://arxiv.org/pdf/1506.01497.pdf) design introduced the concept of a Region Proposal Network (RPN) \
            that takes advantage of the shared feature maps extracted by the CNN backbone to propose object locations in a single, fast end-to-end network. Like many modern \
            detectors, RetinaNet adopts the concept of _anchor boxes_ on FPN as introduced by RPN."
    )

    with st.beta_expander("How do RPNs work?"):

        st.write("**Sliding-window Anchor Grid**")
        st.write(
            "RPN's generate proposals by overlaying a grid of anchor points onto a feature map output by the backbone network where each point corresponds to the center of one activation. \
            A set of anchor boxes is then slid over each point in the anchor grid to capture objects of variable size, height, and width. In particular, RetinaNet captures 9 anchor boxes per spatial \
            location by permuting the anchor box size and aspect ratios. This means that for the 6x6 feature map below, the RPN will generate 6x6x9 = 324 object proposal boxes."
        )
        st.image("images/anchor_explain.png")
        st.write("**RPN for FPN**")
        st.write(
            "As learned in the previous step, RetinaNet uses an FPN with 5 pyramid levels to extract multi-scale features. To accommodate this, RPN independently applys a separate anchor grid to the feature maps \
            generated by each level. The anchors have sizes ranging from 32$^2$ to 512$^2$ pixels across pyramid levels P$_3$ - P$_7$ respectively. This design allows lower level (higher dimensional) feature maps \
            to capture smaller objects (by pixel size), while larger objects are detected from higher level features."
        )
        st.write("**Classification and Box Regression Subnets**")
        st.write(
            "RetinaNet attaches two independent, fully convolutional networks to the outputs of each FPN level. Each subnet shares parameters across all levels. In the classification subnet, each anchor box is responsible \
            for detecting the existence of _at most_ one object from N classes in the spatial region the anchor box covers. Similarly, in the box regression subnet, each anchor box is responsible for detecting the size and \
            shape of _at most_ one object (if any exist) by regressing the relative offset of an object's bounding box from the anchor box."
        )
        pass

    with st.beta_expander("RPN Anchor Grid by Pyramid Level"):
        pyramid_level = st.select_slider(
            label="Select a Feature Pyramid Level:",
            options=[f"P{i+3}" for i in range(5)],
        )

        st.image(session_state.img_paths["rpn"][pyramid_level])

    return
